{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05156311-c402-4898-b89d-f8696f5c4589",
   "metadata": {},
   "source": [
    "# Day 2: Pathology Identification with off-the-shelf LLMs and RAG Augmentation\n",
    "\n",
    "- Task: Identify pathologies from radiology reports\n",
    "\n",
    "## Details\n",
    "\n",
    "- Input: Raw radiology report sections (findings sections) and RAG-retrieved examples\n",
    "- Output: Predicted pathology DISEASE_COLUMNS (multi-label classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d595477-d98e-4f79-9e02-7f166ee19d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# URL:PORT must be identical to what is set in LM Studio!\n",
    "HOST_URL = \"http://localhost:1235\"\n",
    "\n",
    "# model name as served by LM Studio for classification and \n",
    "# inference of embeddings for vectorstore\n",
    "MODEL = \"lmstudio-community/medgemma-4b-it-GGUF\"\n",
    "\n",
    "# model supporting embedding endpoint for vectorized embeddings\n",
    "EMBED_MODEL = \"amsaravi/MedEmbed-large-v0.1.gguf\"\n",
    "\n",
    "# path to logged results\n",
    "Y_PRED_LLM_W_RAG_CACHED = Path('log') / 'y_pred' / 'y_pred_llm_rag.csv'\n",
    "\n",
    "# create parent folder if not existing\n",
    "Y_PRED_LLM_W_RAG_CACHED.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd023fde-4943-4e2b-9842-4fb630dc310a",
   "metadata": {},
   "source": [
    "## Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52bd75ad-1982-4398-a45c-4e3f0c41d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test dim:\t(9603, 2)\ty_test dim:\t(9603, 14)\n",
      "X_test dim:\t(9603, 2)\ty_test dim:\t(9603, 14)\n",
      "X_train dim:\t(38601, 2)\ty_train dim:\t(38601, 14)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "## Load splits\n",
    "def load_test_splits():\n",
    "    data_path = Path(\"data\")\n",
    "    X_test = pd.read_csv(data_path / \"X_test.csv\")\n",
    "    y_test = pd.read_csv(data_path / \"y_test.csv\")\n",
    "    print(f\"X_test dim:\\t{X_test.shape}\\ty_test dim:\\t{y_test.shape}\")\n",
    "    return X_test, y_test\n",
    "\n",
    "## Load splits\n",
    "# TODO: generalize load_test_splits to take the split () as an argument and return either set\n",
    "def load_data_splits(split:str):\n",
    "    assert split in ['test', 'train']\n",
    "    data_path = Path(\"data\")\n",
    "    X = pd.read_csv(data_path / f\"X_{split}.csv\")\n",
    "    y = pd.read_csv(data_path / f\"y_{split}.csv\")\n",
    "    print(f\"X_{split} dim:\\t{X.shape}\\ty_{split} dim:\\t{y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "X_test, y_test = load_test_splits()\n",
    "\n",
    "X_test, y_test = load_data_splits(split='test')\n",
    "X_train, y_train = load_data_splits(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "541cfe48-3f09-464f-bdbf-3a86c9fdc961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deid_patient_id</th>\n",
       "      <th>section_findings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient04528</td>\n",
       "      <td>Unchanged position of the left upper extremity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient04986</td>\n",
       "      <td>12 mm focal density in the region of the anter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient05496</td>\n",
       "      <td>A single AP upright view of the chest taken on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient05496</td>\n",
       "      <td>Compared to prior chest x-ray on 1-16-2019, PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patient05496</td>\n",
       "      <td>Compared to prior chest x-ray on February 15th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deid_patient_id                                   section_findings\n",
       "0    patient04528  Unchanged position of the left upper extremity...\n",
       "1    patient04986  12 mm focal density in the region of the anter...\n",
       "2    patient05496  A single AP upright view of the chest taken on...\n",
       "3    patient05496  Compared to prior chest x-ray on 1-16-2019, PA...\n",
       "4    patient05496  Compared to prior chest x-ray on February 15th..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52ef4db6-c85f-4217-98ff-5c6e477ad954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unchanged position of the left upper extremity PICC line. Again seen \\nare surgical clips projecting over the right hemithorax. The \\ncardiomediastinal silhouette is stable in appearance. Increased \\nstranding opacities are noted in the left retrocardiac region. Subtle \\nstranding opacities in the right upper lung zone are unchanged.. \\nThere are no pleural or significant bony abnormalities. Absence of \\nthe right breast shadow compatible with prior mastectomy.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['section_findings'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "327359b8-d82c-47e8-ab22-bdea6efe29c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
       "0                           0             0             1            0      0   \n",
       "1                           0             0             0            0      0   \n",
       "2                           0             1             0            0      1   \n",
       "3                           0             1             1            0      0   \n",
       "4                           0             1             1            0      0   \n",
       "\n",
       "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
       "0              0          0            0             0                 0   \n",
       "1              0          0            0             0                 0   \n",
       "2              0          0            0             0                 0   \n",
       "3              0          0            0             0                 1   \n",
       "4              0          0            0             0                 1   \n",
       "\n",
       "   Pleural Other  Fracture  Support Devices  No Finding  \n",
       "0              0         0                1           0  \n",
       "1              0         0                0           0  \n",
       "2              0         0                0           0  \n",
       "3              0         0                0           0  \n",
       "4              0         0                0           0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4da7fdc9-d6ca-4859-a0e0-8c002f61b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE_COLUMNS = y_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d5739-bf90-4e35-b4e1-023b85f28d5c",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "For a multi-class, multi-label problem (where true negative (TN) counts are not sensible) suitable metrics are\n",
    "- precision (fraction of correctly capture TPs: TP/(TP + FP))\n",
    "- recall (fraction of recalled TPs: TP/P)\n",
    "- F1 (harmonic mean of precision, recall)\n",
    "\n",
    "There are three distinct strategies on how to combine per class performance:\n",
    "1. micro - global pooling of TP, FP, FN (global picture, bias towards majority classes)\n",
    "2. macro - per class scores are averaged (no bias, minority class sensitivity)\n",
    "3. weighted - per class scores are weighted and averaged (bias towards large classes, moderate impact of minor classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf7a2bd8-1518-4783-83e8-6e96d17b93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(y_true:pd.DataFrame, y_pred:pd.DataFrame, average:str='micro'):\n",
    "    # average methods: \n",
    "    #  micro - global pooling of TP, FP, FN\n",
    "    #  macro - per class scores are averaged \n",
    "    #  weighted - per class scores are weighted and averaged\n",
    "    # Ensure identically ordered columns and numerical type\n",
    "    y_true = y_true[DISEASE_COLUMNS].astype(int)\n",
    "    y_pred = y_pred[DISEASE_COLUMNS].astype(int)\n",
    "\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return pd.DataFrame({f\"{average}-F1\": [f1], \n",
    "                        f\"{average}-Precision\": [precision],\n",
    "                        f\"{average}-Recall\": [recall]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc96631-6d13-4b04-b307-75134ad1cac0",
   "metadata": {},
   "source": [
    "## Build Vectorstore from Train Set\n",
    "### Client for Embedding Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6042a35-1a2f-451f-a750-a8a10ae52972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class LMStudioEmbeddingClient:\n",
    "    def __init__(self, base_url=HOST_URL, model_name=EMBED_MODEL):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.model_name = model_name\n",
    "        self.batch_id_state = Path(\"current_batch_id.txt\")\n",
    "\n",
    "\n",
    "    def encode_batch(self, texts):\n",
    "        \"\"\"One-shot encoding for small lists (e.g. single query).\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        payload = {\"model\": self.model_name, \"input\": texts}\n",
    "        resp = requests.post(f\"{self.base_url}/v1/embeddings\", json=payload)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        emb = np.array(\n",
    "            [item[\"embedding\"] for item in data[\"data\"]],\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        return emb  # shape (len(texts), dim)\n",
    "\n",
    "    def encode(self, texts, batch_size=128):\n",
    "        \"\"\"\n",
    "        Generator: yields one np.ndarray of embeddings per batch.\n",
    "        Resumes from the last completed batch using current_batch_id.txt.\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        # Determine starting batch index\n",
    "        start_batch_id = 0\n",
    "        if self.batch_id_state.exists():\n",
    "            with open(self.batch_id_state, \"r\") as f:\n",
    "                line = f.readline().strip()\n",
    "                if line:\n",
    "                    start_batch_id = int(line) + 1  # resume AFTER last finished batch\n",
    "\n",
    "        indices = list(range(0, len(texts), batch_size))\n",
    "\n",
    "        for batch_id, start in enumerate(indices):\n",
    "            if batch_id < start_batch_id:\n",
    "                continue  # skip already processed batches\n",
    "\n",
    "            end = start + batch_size\n",
    "            print(f\"INFO\\tProcessing batch {batch_id} ({start}:{end}) \"\n",
    "                  f\"of total {len(texts)} items.\")\n",
    "\n",
    "            batch = texts[start:end]\n",
    "            payload = {\"model\": self.model_name, \"input\": batch}\n",
    "            resp = requests.post(f\"{self.base_url}/v1/embeddings\", json=payload)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            batch_emb = np.array(\n",
    "                [item[\"embedding\"] for item in data[\"data\"]],\n",
    "                dtype=\"float32\",\n",
    "            )\n",
    "\n",
    "            # Persist last completed batch id\n",
    "            with open(self.batch_id_state, \"w\") as f:\n",
    "                f.write(str(batch_id))\n",
    "\n",
    "            yield batch_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d052d7-da43-4526-8228-914bbc8d3465",
   "metadata": {},
   "source": [
    "### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac0673e-b5f2-44da-b18b-105e976ef1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index from cache and appending...\n",
      "STATUS\tLoaded FAISS index with 77202 embeddings.\n",
      "STATUS\tIndex now has 115803 entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Vectorstore:\n",
    "    def __init__(self, X_train, y_train, embedding_client, cache_dir=\"vectorstore\"):\n",
    "        \"\"\"\n",
    "        Initialize Vectorstore with training data and an embedding client.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Must have columns ['deid_patient_id', 'section_findings'].\n",
    "            y_train (np.ndarray or pd.DataFrame): Multi-hot encoded or equivalent label matrix.\n",
    "            embedding_client: Object with an .encode(list[str]) -> np.ndarray method (e.g. LMStudioEmbeddingClient).\n",
    "        \n",
    "            cache_dir (str): Directory for cached FAISS index and metadata.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.embedding_client = embedding_client\n",
    "\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # File paths\n",
    "        self.index_file = self.cache_dir / \"faiss_index.bin\"\n",
    "        self.metadata_file = self.cache_dir / \"metadata.pkl\"\n",
    "\n",
    "        # Placeholders\n",
    "        self.faiss_index = None\n",
    "        self.train_texts = None\n",
    "        self.train_labels = None\n",
    "\n",
    "        # Build vectorstore\n",
    "        # self.batch_id = 0\n",
    "        \n",
    "        if self.faiss_index is None:\n",
    "            self.build_vector_store(self.X_train[\"section_findings\"].tolist(), self.y_train)\n",
    "            \n",
    "\n",
    "    def retrieve_similar_cases(self, query_text, k=5):\n",
    "        \"\"\"Retrieve k most similar cases; build index if not available.\"\"\"\n",
    "    \n",
    "        # One-shot embedding for a single query\n",
    "        query_embedding = self.embedding_client.encode_batch([query_text])  # (1, dim)\n",
    "        query_embedding = np.asarray(query_embedding, dtype=\"float32\")\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "        scores, indices = self.faiss_index.search(query_embedding, k)\n",
    "    \n",
    "        similar_texts = [self.train_texts[i] for i in indices[0]]\n",
    "        similar_labels = [self.train_labels.iloc[i] for i in indices[0]]\n",
    "        similar_scores = scores[0]\n",
    "    \n",
    "        return similar_texts, similar_labels, similar_scores\n",
    "\n",
    "    \n",
    "\n",
    "    def build_vector_store(self, texts, labels, force_rebuild=False, batch_size=128):\n",
    "        \"\"\"Build or extend FAISS index incrementally from batches, persisting after each batch.\"\"\"\n",
    "        if self._cache_exists() and not force_rebuild:\n",
    "            print(\"Loading FAISS index from cache and appending...\")\n",
    "            self._load_cache()\n",
    "        else:\n",
    "            print(\"Building new FAISS index from reference set...\")\n",
    "            self.faiss_index = None\n",
    "            self.train_texts = []\n",
    "            self.train_labels = None\n",
    "    \n",
    "        # Append new texts/labels to metadata\n",
    "        self.train_texts.extend(texts)\n",
    "        if self.train_labels is None:\n",
    "            self.train_labels = labels\n",
    "        else:\n",
    "            # self.train_labels = self.train_labels.append(labels, ignore_index=True)\n",
    "            self.train_labels = pd.concat([self.train_labels, labels], ignore_index=True)\n",
    "    \n",
    "        # Stream embeddings in batches and add to FAISS, persisting each time\n",
    "        for batch_emb in self.embedding_client.encode(texts, batch_size=batch_size):\n",
    "            faiss.normalize_L2(batch_emb)\n",
    "    \n",
    "            if self.faiss_index is None:\n",
    "                dim = batch_emb.shape[1]\n",
    "                self.faiss_index = faiss.IndexFlatIP(dim)\n",
    "    \n",
    "            self.faiss_index.add(batch_emb)\n",
    "    \n",
    "            # Persist index + metadata after each batch\n",
    "            self._save_cache()\n",
    "            print(f\"STATUS\\tPersisted index with {self.faiss_index.ntotal} vectors.\")\n",
    "    \n",
    "        print(f\"STATUS\\tIndex now has {len(self.train_texts)} entries.\")\n",
    "    \n",
    "    \n",
    "    def _cache_exists(self):\n",
    "        return self.index_file.exists() and self.metadata_file.exists()\n",
    "\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save FAISS index and metadata; embeddings are stored inside faiss_index.\"\"\"\n",
    "        faiss.write_index(self.faiss_index, str(self.index_file))\n",
    "        metadata = {\n",
    "            \"train_texts\": self.train_texts,\n",
    "            \"train_labels\": self.train_labels,\n",
    "            \"ntotal\": self.faiss_index.ntotal,\n",
    "            # optionally store model id instead of path if you switched to LM Studio\n",
    "            # \"embedding_model_id\": self.embedding_model_id,\n",
    "        }\n",
    "        with open(self.metadata_file, \"wb\") as f:\n",
    "            pickle.dump(metadata, f)\n",
    "\n",
    "\n",
    "    def _load_cache(self):\n",
    "        self.faiss_index = faiss.read_index(str(self.index_file))\n",
    "        with open(self.metadata_file, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        self.train_texts = metadata['train_texts']\n",
    "        self.train_labels = metadata['train_labels']\n",
    "        print(f\"STATUS\\tLoaded FAISS index with {len(self.train_texts)} embeddings.\")\n",
    "\n",
    "# LM Studio model id as configured in the app\n",
    "embedding_client = LMStudioEmbeddingClient(\n",
    "    base_url=HOST_URL,  # LM Studio API URL\n",
    "    model_name=EMBED_MODEL,\n",
    ")\n",
    "\n",
    "vectorstore = Vectorstore(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    embedding_client=embedding_client,\n",
    "    cache_dir=\"vectorstore\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418a013-98de-4b5e-9794-9a1ba9b2bfb1",
   "metadata": {},
   "source": [
    "## Label Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67a22861-a8f7-4da4-b2b5-751eba6ae411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "\n",
    "def extract_json_or_list(text_with_json: str):\n",
    "    # Regex matches both lists ([...]) and dicts ({...})\n",
    "    json_rx = re.compile(r\"(\\{.*?\\}|\\[.*?\\])\", re.DOTALL)\n",
    "    matches = json_rx.findall(text_with_json)\n",
    "    if not matches:\n",
    "        warnings.warn(f\"Could not extract JSON/list block: {text_with_json}\")\n",
    "        return None\n",
    "    last_json = matches[-1]\n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        parsed = json.loads(last_json)\n",
    "        return parsed\n",
    "    except json.JSONDecodeError as e:\n",
    "        warnings.warn(\n",
    "            f\"Could not decode JSON/list: {e}\\nRaw block: {last_json}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "def cleanse_to_multihot(json_or_list, all_labels=DISEASE_COLUMNS):\n",
    "    # Case 1: 0/1 or True/False dict\n",
    "    if isinstance(json_or_list, dict):\n",
    "        filtered_pred = {}\n",
    "        for label in all_labels:\n",
    "            value = json_or_list.get(label, 0)\n",
    "            if isinstance(value, (int, float, bool)):\n",
    "                filtered_pred[label] = 1 if value else 0\n",
    "            elif isinstance(value, str):\n",
    "                filtered_pred[label] = 1 if value.lower() in {'1', 'true', 'yes'} else 0\n",
    "            else:\n",
    "                filtered_pred[label] = 0\n",
    "        return filtered_pred\n",
    "    # Case 2: list of strings = present labels only\n",
    "    elif isinstance(json_or_list, list):\n",
    "        return {label: 1 if label in json_or_list else 0 for label in all_labels}\n",
    "    # Unrecognized\n",
    "    else:\n",
    "        warnings.warn(\"Unknown prediction format. Returning None.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd86afc6-d004-465e-aed5-3ab24baae720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class GenerativeLLMClassifier():\n",
    "    def __init__(self, model:str='llm'):\n",
    "        \n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        # self.log_path = Path(\"..\") / \"log\"\n",
    "        model_name = model.split(\"/\")[-1]\n",
    "        self.log_path = Path('log') / model_name\n",
    "\n",
    "        self.log_path_set = False\n",
    "        self.client = OpenAI(\n",
    "            base_url=f\"{HOST_URL}/v1\",\n",
    "            api_key='dummy'\n",
    "        )\n",
    "\n",
    "    \n",
    "    def build_prompt(self, query_text, similar_examples=None, k:int=5):\n",
    "        if similar_examples is None:\n",
    "            similar_examples = []\n",
    "        # Create prompt optionally RAG-augmented with retrieved examples\n",
    "     \n",
    "        prompt = f\"\"\"You are a radiology AI assistant. Classify the following medical text for pathologies.\n",
    "        ### Task\n",
    "        Determine which of these pathologies are present: [{', '.join(DISEASE_COLUMNS)}]\n",
    "        \"\"\"\n",
    "    \n",
    "        if similar_examples:\n",
    "            # Add retrieved examples\n",
    "            prompt += \"### Similar Examples from Training Data:\"\n",
    "            similar_examples = list(similar_examples)\n",
    "            for i, (text, labels) in enumerate(similar_examples[:k]):\n",
    "                positive_labels = [label for label, value in zip(DISEASE_COLUMNS, labels) if value == 1]\n",
    "                prompt += f\"\"\"\n",
    "            Example {i+1}:\n",
    "            Text: \"{text}\"\n",
    "            Present pathologies: {', '.join(positive_labels) if positive_labels else 'No Finding'}\n",
    "            \"\"\"\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "        ### Your Task\n",
    "        Text to classify: \"{query_text}\"\n",
    "        \n",
    "        Return JSON with 0/1 for each pathology:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    \n",
    "    def run(self, text_id, query, vectorstore=None, k=5):\n",
    "        if self.log_path_set == False:\n",
    "            path_str = f\"{self.timestamp}_rag\" if vectorstore else f\"{self.timestamp}_no_rag\"\n",
    "            self.log_path = self.log_path / path_str\n",
    "            self.log_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.log_path_set = True\n",
    "        if vectorstore:\n",
    "            similar_texts, similar_labels, scores = vectorstore.retrieve_similar_cases(query, k=k)\n",
    "            user_prompt = self.build_prompt(query, zip(similar_texts, similar_labels), k)\n",
    "        else:\n",
    "            user_prompt = self.build_prompt(query)\n",
    "\n",
    "        # Log prompt\n",
    "        with open(self.log_path / f\"{text_id}_prompt.log\", 'w') as f:\n",
    "            f.write(user_prompt)\n",
    "\n",
    "        # Generate a prompt completion\n",
    "        system_prompt = \"You are a clinical NLP assistant specialized in radiology.\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        # params = self._cfg[\"params\"] or {}\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=MODEL, # only mandatory if you serve multiple models in LM Studio!\n",
    "            messages=messages, #**params\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "        \n",
    "\n",
    "llm_classifier = GenerativeLLMClassifier(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4164a-2ba3-4cca-b9ef-20a43cf3c1c9",
   "metadata": {},
   "source": [
    "## Classifier with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77e27d4e-ab71-4df6-b871-539fa3db6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "def classify_all(classifier, X_test, y_test, acc, vectorstore=None):\n",
    "    \"\"\"\n",
    "    Appends new predictions for X_test/y_test rows with indices not in acc, to acc DataFrame.\n",
    "\n",
    "    Args:\n",
    "        classifier: LLM classifier instance (with .run method)\n",
    "        X_test (pd.DataFrame): Test set with 'section_findings' column\n",
    "        y_test (pd.DataFrame): True labels DataFrame for test set\n",
    "        acc (pd.DataFrame): Accumulator DataFrame of prior predictions (index = text_id)\n",
    "        vectorstore: Optional retrieval model\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated accumulator DataFrame (with new predictions appended)\n",
    "    \"\"\"\n",
    "\n",
    "    new_indices = []\n",
    "    new_preds = []\n",
    "\n",
    "    if acc is None:\n",
    "        acc = pd.DataFrame(columns=DISEASE_COLUMNS)\n",
    "\n",
    "    for text_id, row in X_test.iterrows():\n",
    "        if text_id in acc.index:\n",
    "            print(f\"Skipping text_id {text_id}: already in accumulator.\")\n",
    "            continue\n",
    "        print(f\"Processing text_id {text_id} ...\")\n",
    "        \n",
    "        text = row[\"section_findings\"]\n",
    "        label_row = y_test.loc[text_id]   # use .loc for index alignment\n",
    "        active_labels = label_row[label_row == 1].index.tolist()\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Active labels: {active_labels}\")\n",
    "        \n",
    "        completion = classifier.run(text_id, text, vectorstore)\n",
    "        logfile = classifier.log_path / f\"{text_id}_completion.log\"\n",
    "        with open(logfile, \"w\") as f:\n",
    "            f.write(completion)\n",
    "        try:\n",
    "            json_or_list = extract_json_or_list(completion)\n",
    "            if json_or_list is None:\n",
    "                print(f\"Skipping text_id {text_id}: could not parse completion.\")\n",
    "                continue\n",
    "            y_pred_row = cleanse_to_multihot(json_or_list, DISEASE_COLUMNS)\n",
    "            new_indices.append(text_id)\n",
    "            new_preds.append(y_pred_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing completion {text_id}: {e}\")\n",
    "\n",
    "    y_pred_new = pd.DataFrame(new_preds, columns=DISEASE_COLUMNS, index=new_indices)\n",
    "    acc_updated = pd.concat([acc, y_pred_new])\n",
    "    return acc_updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8900157c-da2a-4927-93a8-cee4d2703f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_prediction(rag_flag:bool=False):\n",
    "    cache_path = Y_PRED_LLM_W_RAG_CACHED if rag_flag else Y_PRED_LLM_CACHED\n",
    "    if Path(cache_path).exists():\n",
    "        return pd.read_csv(cache_path, index_col=0)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=DISEASE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fecd26f-b9d1-4cba-9abe-6b08a2070bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text_id 0 ...\n",
      "Text: Unchanged position of the left upper extremity PICC line. Again seen \n",
      "are surgical clips projecting over the right hemithorax. The \n",
      "cardiomediastinal silhouette is stable in appearance. Increased \n",
      "stranding opacities are noted in the left retrocardiac region. Subtle \n",
      "stranding opacities in the right upper lung zone are unchanged.. \n",
      "There are no pleural or significant bony abnormalities. Absence of \n",
      "the right breast shadow compatible with prior mastectomy.\n",
      "Active labels: ['Lung Opacity', 'Support Devices']\n",
      "Processing text_id 1 ...\n",
      "Text: 12 mm focal density in the region of the anterior right fifth rib. \n",
      "Lungs are otherwise clear and the cardiac silhouette is not enlarged.\n",
      "Active labels: []\n",
      "Processing text_id 2 ...\n",
      "Text: A single AP upright view of the chest taken on 22 JUNE 2006 \n",
      "demonstrates sternal wires and mediastinal clips in place.  Cardiac \n",
      "silhouette is enlarged but not significantly changed from the prior \n",
      "study.  The aorta is ectatic and tortuous in the chest.  Compared \n",
      "with the previous study, the pulmonary vessels are somewhat more \n",
      "prominent, suggesting a mild degree of pulmonary edema.  Costophrenic \n",
      "sulci are sharp.  No pneumothorax is seen.  No significant bony \n",
      "abnormality is appreciated.\n",
      "Active labels: ['Cardiomegaly', 'Edema']\n",
      "Processing text_id 3 ...\n",
      "Text: Compared to prior chest x-ray on 1-16-2019, PA and \n",
      "lateral upright views of the chest again demonstrate postsurgical \n",
      "changes of the mitral valve replacement and tricuspid valve \n",
      "anuloplasty.   There is stable cardiomegaly.   There is a persistent \n",
      "right pleural effusion and elevation of the right hemidiaphragm which \n",
      "is grossly stable in appearance.   The left pleural effusion is \n",
      "significantly resolved.   There is persistent linear opacities in the \n",
      "left lung base which may represent atelectasis or scarring.   There \n",
      "is stable calcification of thoracoabdominal aorta.   Minimal \n",
      "degenerative disease of the thoracic spine.\n",
      "Active labels: ['Cardiomegaly', 'Lung Opacity', 'Pleural Effusion']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m n = \u001b[32m15\u001b[39m\n\u001b[32m      4\u001b[39m y_pred1 = create_or_load_prediction(rag_flag=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y_pred1 = \u001b[43mclassify_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m y_pred1\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mclassify_all\u001b[39m\u001b[34m(classifier, X_test, y_test, acc, vectorstore)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mActive labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m completion = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m logfile = classifier.log_path / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_completion.log\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(logfile, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mGenerativeLLMClassifier.run\u001b[39m\u001b[34m(self, text_id, query, vectorstore, k)\u001b[39m\n\u001b[32m     73\u001b[39m messages = [\n\u001b[32m     74\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m     75\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_prompt},\n\u001b[32m     76\u001b[39m ]\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# params = self._cfg[\"params\"] or {}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m completion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# only mandatory if you serve multiple models in LM Studio!\u001b[39;49;00m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#**params\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/workshop_llm_classifier/.env/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Restrict computation to the first n items to save time\n",
    "n = 15\n",
    "\n",
    "y_pred1 = create_or_load_prediction(rag_flag=True)\n",
    "y_pred1 = classify_all(llm_classifier, X_test.head(n), y_test, y_pred1, vectorstore)\n",
    "\n",
    "\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64917d-db36-4167-9413-9fb72a69cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that y_test is indexed the same as y_pred\n",
    "y_test_subset = y_test.loc[y_pred1.index]\n",
    "\n",
    "# Calculate scores\n",
    "scores = compute_scores(y_test_subset, y_pred1, average='micro')\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f9a06-45b3-4a59-b88f-892257907db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1.to_csv(Y_PRED_LLM_CACHED, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd004c75-de79-41fc-b72b-c995400793d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
