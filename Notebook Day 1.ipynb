{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05156311-c402-4898-b89d-f8696f5c4589",
   "metadata": {},
   "source": [
    "# Day 1: Pathology Identification with off-the-shelf LLMs\n",
    "\n",
    "- Task: Identify pathologies from radiology reports\n",
    "\n",
    "## Details\n",
    "\n",
    "- Input: Raw radiology report sections (findings sections)\n",
    "- Output: Predicted pathology DISEASE_COLUMNS (multi-label classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d595477-d98e-4f79-9e02-7f166ee19d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# URL:PORT must be identical to what is set in LM Studio!\n",
    "HOST_URL = \"http://localhost:1235/v1\" \n",
    "\n",
    "# model name as served by LM Studio\n",
    "MODEL = 'unsloth/medgemma-4b-it-gguf/medgemma-4b-it-q4_k_s.gguf'\n",
    "\n",
    "# path to logged results\n",
    "Y_PRED_LLM_CACHED = Path('log') / 'y_pred' / 'y_pred_llm_no_rag.csv'\n",
    "\n",
    "# create parent folder if not existing\n",
    "Y_PRED_LLM_CACHED.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd023fde-4943-4e2b-9842-4fb630dc310a",
   "metadata": {},
   "source": [
    "## Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd75ad-1982-4398-a45c-4e3f0c41d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "## Load splits\n",
    "def load_test_splits():\n",
    "    data_path = Path(\"data\")\n",
    "    X_test = pd.read_csv(data_path / \"X_test.csv\")\n",
    "    y_test = pd.read_csv(data_path / \"y_test.csv\")\n",
    "    print(f\"X_test dim:\\t{X_test.shape}\\ty_test dim:\\t{y_test.shape}\")\n",
    "    return X_test, y_test\n",
    "\n",
    "X_test, y_test = load_test_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541cfe48-3f09-464f-bdbf-3a86c9fdc961",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef4db6-c85f-4217-98ff-5c6e477ad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['section_findings'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327359b8-d82c-47e8-ab22-bdea6efe29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7fdc9-d6ca-4859-a0e0-8c002f61b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE_COLUMNS = y_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d5739-bf90-4e35-b4e1-023b85f28d5c",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "For a multi-class, multi-label problem (where true negative (TN) counts are not sensible) suitable metrics are\n",
    "- precision (fraction of correctly capture TPs: TP/(TP + FP))\n",
    "- recall (fraction of recalled TPs: TP/P)\n",
    "- F1 (harmonic mean of precision, recall)\n",
    "\n",
    "There are three distinct strategies on how to combine per class performance:\n",
    "1. micro - global pooling of TP, FP, FN (global picture, bias towards majority classes)\n",
    "2. macro - per class scores are averaged (no bias, minority class sensitivity)\n",
    "3. weighted - per class scores are weighted and averaged (bias towards large classes, moderate impact of minor classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a2bd8-1518-4783-83e8-6e96d17b93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(y_true:pd.DataFrame, y_pred:pd.DataFrame, average:str='micro'):\n",
    "    # average methods: \n",
    "    #  micro - global pooling of TP, FP, FN\n",
    "    #  macro - per class scores are averaged \n",
    "    #  weighted - per class scores are weighted and averaged\n",
    "    # Ensure identically ordered columns and numerical type\n",
    "    y_true = y_true[DISEASE_COLUMNS].astype(int)\n",
    "    y_pred = y_pred[DISEASE_COLUMNS].astype(\"Int64\").fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return pd.DataFrame({f\"{average}-F1\": [f1], \n",
    "                        f\"{average}-Precision\": [precision],\n",
    "                        f\"{average}-Recall\": [recall]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a22861-a8f7-4da4-b2b5-751eba6ae411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "\n",
    "def extract_json_or_list(text_with_json: str):\n",
    "    # Regex matches both lists ([...]) and dicts ({...})\n",
    "    json_rx = re.compile(r\"(\\{.*?\\}|\\[.*?\\])\", re.DOTALL)\n",
    "    matches = json_rx.findall(text_with_json)\n",
    "    if not matches:\n",
    "        warnings.warn(f\"Could not extract JSON/list block: {text_with_json}\")\n",
    "        return None\n",
    "    last_json = matches[-1]\n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        parsed = json.loads(last_json)\n",
    "        return parsed\n",
    "    except json.JSONDecodeError as e:\n",
    "        warnings.warn(\n",
    "            f\"Could not decode JSON/list: {e}\\nRaw block: {last_json}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "def cleanse_to_multihot(json_or_list, all_labels=DISEASE_COLUMNS):\n",
    "    # Case 1: 0/1 or True/False dict\n",
    "    if isinstance(json_or_list, dict):\n",
    "        filtered_pred = {}\n",
    "        for label in all_labels:\n",
    "            value = json_or_list.get(label, 0)\n",
    "            if isinstance(value, (int, float, bool)):\n",
    "                filtered_pred[label] = 1 if value else 0\n",
    "            elif isinstance(value, str):\n",
    "                filtered_pred[label] = 1 if value.lower() in {'1', 'true', 'yes'} else 0\n",
    "            else:\n",
    "                filtered_pred[label] = 0\n",
    "        return filtered_pred\n",
    "    # Case 2: list of strings = present labels only\n",
    "    elif isinstance(json_or_list, list):\n",
    "        return {label: 1 if label in json_or_list else 0 for label in all_labels}\n",
    "    # Unrecognized\n",
    "    else:\n",
    "        warnings.warn(\"Unknown prediction format. Returning None.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86afc6-d004-465e-aed5-3ab24baae720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class GenerativeLLMClassifier():\n",
    "    def __init__(self, model:str='llm'):\n",
    "        \n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        # self.log_path = Path(\"..\") / \"log\"\n",
    "        model_name = model.split(\"/\")[-1]\n",
    "        self.log_path = Path('log') / model_name\n",
    "\n",
    "        self.log_path_set = False\n",
    "        self.client = OpenAI(\n",
    "            base_url=HOST_URL,\n",
    "            api_key='dummy'\n",
    "        )\n",
    "\n",
    "    \n",
    "    def build_prompt(self, query_text, similar_examples=None, k:int=5):\n",
    "        if similar_examples is None:\n",
    "            similar_examples = []\n",
    "        # Create prompt optionally RAG-augmented with retrieved examples\n",
    "     \n",
    "        prompt = f\"\"\"You are a radiology AI assistant. Classify the following medical text for pathologies.\n",
    "        ### Task\n",
    "        Determine which of these pathologies are present: [{', '.join(DISEASE_COLUMNS)}]\n",
    "        \"\"\"\n",
    "    \n",
    "        if similar_examples:\n",
    "            # Add retrieved examples\n",
    "            prompt += \"### Similar Examples from Training Data:\"\n",
    "            similar_examples = list(similar_examples)\n",
    "            for i, (text, labels) in enumerate(similar_examples[:k]):\n",
    "                positive_labels = [label for label, value in zip(DISEASE_COLUMNS, labels) if value == 1]\n",
    "                prompt += f\"\"\"\n",
    "            Example {i+1}:\n",
    "            Text: \"{text}\"\n",
    "            Present pathologies: {', '.join(positive_labels) if positive_labels else 'No Finding'}\n",
    "            \"\"\"\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "        ### Your Task\n",
    "        Text to classify: \"{query_text}\"\n",
    "        \n",
    "        Return JSON with 0/1 for each pathology:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    \n",
    "    def run(self, text_id, query, vectorstore=None, k=5):\n",
    "        if self.log_path_set == False:\n",
    "            path_str = f\"{self.timestamp}_rag\" if vectorstore else f\"{self.timestamp}_no_rag\"\n",
    "            self.log_path = self.log_path / path_str\n",
    "            self.log_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.log_path_set = True\n",
    "\n",
    "        if vectorstore:\n",
    "            similar_texts, similar_labels, scores = vectorstore.retrieve_similar_cases(query, k=k)\n",
    "            user_prompt = self.build_prompt(query, zip(similar_texts, similar_labels), k)\n",
    "        else:\n",
    "            user_prompt = self.build_prompt(query)\n",
    "\n",
    "        # Log prompt\n",
    "        with open(self.log_path / f\"{text_id}_prompt.log\", 'w') as f:\n",
    "            f.write(user_prompt)\n",
    "\n",
    "        # Generate a prompt completion\n",
    "        system_prompt = \"You are a clinical NLP assistant specialized in radiology.\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        # params = self._cfg[\"params\"] or {}\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=MODEL, # only mandatory if you serve multiple models in LM Studio!\n",
    "            messages=messages, #**params\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "        \n",
    "\n",
    "llm_classifier = GenerativeLLMClassifier(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4164a-2ba3-4cca-b9ef-20a43cf3c1c9",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e27d4e-ab71-4df6-b871-539fa3db6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "def classify_all(classifier, X_test, y_test, acc, vectorstore=None):\n",
    "    \"\"\"\n",
    "    Appends new predictions for X_test/y_test rows with indices not in acc, to acc DataFrame.\n",
    "\n",
    "    Args:\n",
    "        classifier: LLM classifier instance (with .run method)\n",
    "        X_test (pd.DataFrame): Test set with 'section_findings' column\n",
    "        y_test (pd.DataFrame): True labels DataFrame for test set\n",
    "        acc (pd.DataFrame): Accumulator DataFrame of prior predictions (index = text_id)\n",
    "        vectorstore: Optional retrieval model\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated accumulator DataFrame (with new predictions appended)\n",
    "    \"\"\"\n",
    "\n",
    "    new_indices = []\n",
    "    new_preds = []\n",
    "\n",
    "    if acc is None:\n",
    "        acc = pd.DataFrame(columns=DISEASE_COLUMNS)\n",
    "\n",
    "    for text_id, row in X_test.iterrows():\n",
    "        if text_id in acc.index:\n",
    "            print(f\"Skipping text_id {text_id}: already in accumulator.\")\n",
    "            continue\n",
    "        print(f\"Processing text_id {text_id} ...\")\n",
    "        \n",
    "        text = row[\"section_findings\"]\n",
    "        label_row = y_test.loc[text_id]   # use .loc for index alignment\n",
    "        active_labels = label_row[label_row == 1].index.tolist()\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Active labels: {active_labels}\")\n",
    "        \n",
    "        completion = classifier.run(text_id, text, vectorstore)\n",
    "        logfile = classifier.log_path / f\"{text_id}_completion.log\"\n",
    "        with open(logfile, \"w\") as f:\n",
    "            f.write(completion)\n",
    "        try:\n",
    "            json_or_list = extract_json_or_list(completion)\n",
    "            if json_or_list is None:\n",
    "                print(f\"Skipping text_id {text_id}: could not parse completion.\")\n",
    "                continue\n",
    "            y_pred_row = cleanse_to_multihot(json_or_list, DISEASE_COLUMNS)\n",
    "            new_indices.append(text_id)\n",
    "            new_preds.append(y_pred_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing completion {text_id}: {e}\")\n",
    "\n",
    "    y_pred_new = pd.DataFrame(new_preds, columns=DISEASE_COLUMNS, index=new_indices)\n",
    "    acc_updated = pd.concat([acc, y_pred_new])\n",
    "    return acc_updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900157c-da2a-4927-93a8-cee4d2703f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_prediction(rag_flag:bool=False):\n",
    "    cache_path = Y_PRED_LLM_W_RAG_CACHED if rag_flag else Y_PRED_LLM_CACHED\n",
    "    if Path(cache_path).exists():\n",
    "        return pd.read_csv(cache_path, index_col=0)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=DISEASE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fecd26f-b9d1-4cba-9abe-6b08a2070bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict computation to the first n items to save time\n",
    "n = 5\n",
    "\n",
    "y_pred1 = create_or_load_prediction(rag_flag=False)\n",
    "y_pred1 = classify_all(llm_classifier, X_test.head(n), y_test, y_pred1)\n",
    "\n",
    "\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64917d-db36-4167-9413-9fb72a69cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that y_test is indexed the same as y_pred\n",
    "y_test_subset = y_test.loc[y_pred1.index]\n",
    "\n",
    "# Calculate scores\n",
    "scores = compute_scores(y_test_subset, y_pred1, average='micro')\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f9a06-45b3-4a59-b88f-892257907db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1.to_csv(Y_PRED_LLM_CACHED, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
